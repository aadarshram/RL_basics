Q-learning:
--------------------------------------------------------------------------------------------------------------------------------
It is value-based, model-free, off-policy, based on temporal difference method and is tabular.

A new term called Q-function is introduced which determines the quality of taking an action from a given state.
Hence, V π = ∑π (a|s) Qπ (s, a)

The method uses a Q-table that stores Q-values of all possible state action pairs.

The updates to the Q-values while learning follow-
Q(st, at) = Q(st, at) + α . [r t + γ . max Q(st + 1, a) − Q (st, at)]
where r t + gamma . max Q(st + 1, a) is an estimated target Q-value. The method of making such estimate is called bootstrapping.
and the difference is called the Temporal Difference (TD) error.

Upon many iterations, the goal is to end up with the optimal Q values which result in maximum reward.

Epsilon-greedy method - This method is used to balance the tradeoff between exploration and exploitation. The variable epsilon is set to a small number (<0.5) and a number x is randomly generated. If x<epsilon, the algorithm explores else it exploits using the Q table to sample the best action.

Q-learning algorithm - 

Initialize Q-table arbitrarily
Repeat for each episode
    Initialise state
    Repeat each step of episode
        Choose action given state using epsilon-greedy policy
        Take the action and observe next state and reward
        Update Q-value based on that
        Update state

One of the biggest drawbacks of Q-learning method is its inability to scale to bigger problems with more states and actions and also learn in environments which involves continuous states and actions. This is because it uses finite-size tables for learning.

-----------------------------------------------------------------------------------------------------
Additional improvements include -

Decaying epsilon - 
Can be linear, exponential, inverse (decreases as time increases), piecewise constant, adaptive (modify based on relative performance)
Adaptive learning rate - 
Can be step decay, exponential decay, inverse time, adaptive (based on gradient's moments)
----------------------------------------------------------------------------------------------------------